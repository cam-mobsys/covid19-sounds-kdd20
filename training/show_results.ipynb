{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Results for Each Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the evaluation process. Extracted features are utilized as the input of SVM, and different metrics are reported to showcase the effectiveness of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.externals import joblib #import joblib #if externals does not exist\n",
    "from sklearn import decomposition\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def RandomUnderSampler(np_data, np_label): \n",
    "    \"\"\"downsample the majority class according to the given labels.\n",
    "\n",
    "    :param np_data: extracted features as a array\n",
    "    :type np_data: numpy.ndarray\n",
    "    :param np_label: correspoinds labes as a vector\n",
    "    :type np_data: numpy.ndarray\n",
    "    :return: feature vectors and labels for balanced samples\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    label = list(set(np_label))\n",
    "    \n",
    "    # perform a sanity check\n",
    "    if len(label) < 2:\n",
    "        raise ValueError(\"Less than two classed input\")\n",
    "        \n",
    "    # seperate two class\n",
    "    number_c0 = np.sum(np_label == label[0])\n",
    "    number_c1 = np.sum(np_label == label[1])\n",
    "    x_c0 = np_data[np_label == label[0], :]\n",
    "    x_c1 = np_data[np_label == label[1], :]\n",
    "    y_c0 = np_label[np_label == label[0]]\n",
    "    y_c1 = np_label[np_label == label[1]]\n",
    "    \n",
    "    # downsample the majority class\n",
    "    random.seed(0)\n",
    "    if number_c0 < number_c1:\n",
    "        index = random.sample(range(0, number_c1), number_c0)\n",
    "        x_c1 = x_c1[index, :]\n",
    "        y_c1 = y_c1[index]\n",
    "\n",
    "    else:\n",
    "        index = random.sample(range(0, number_c0), number_c1)\n",
    "        x_c0 = x_c0[index, :]\n",
    "        y_c0 = y_c0[index]\n",
    "        \n",
    "    new_data = np.concatenate((x_c0, x_c1), axis=0)\n",
    "    new_label = np.concatenate((y_c0, y_c1), axis=0)\n",
    "    \n",
    "    #return the balanced class\n",
    "    return new_data, new_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Type 3, (A) denotes that we use VGGish-based feature plus duration, tempo, onset, and period.\n",
    "### Parameter: Cough+Breath with PCA =0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========feature group=======\n",
      "features dimension: 260\n",
      "PCA: 0.95\n",
      "Conduct task1\n",
      "Tasks balanced0.95task1\n",
      "Train 219.80(22.72)\n",
      "Test 60.80(20.28)\n",
      "BreathingCough_PCA 118.20(6.84)\n",
      "BreathingCough_AUC 0.7971(0.0688)\n",
      "BreathingCough_ACC 0.7059(0.0588)\n",
      "BreathingCough_Pre 0.7163(0.0612)\n",
      "BreathingCough_Rec 0.6864(0.1128)\n"
     ]
    }
   ],
   "source": [
    "for features in [4]:  # different feature combination\n",
    "    print(\"========feature group=======\")\n",
    "    Number = features + 256  # the feature dimension for one modal\n",
    "    print(\"features dimension:\", Number)\n",
    "    x_data_all_1 = np.load(\"x_data_handcraft.npy\", allow_pickle=True)\n",
    "    x_data_all_2 = np.load(\"x_data_vgg.npy\", allow_pickle=True)\n",
    "    x_data_all_2 = np.squeeze(x_data_all_2)  # breath all, cough all, demo\n",
    "    # subset from handcraft for breath + vgg for breath + subset from handcraft for cough + vgg for cough\n",
    "    x_data_all = np.concatenate(\n",
    "        (\n",
    "            x_data_all_1[:, :features],\n",
    "            x_data_all_2[:, :256],\n",
    "            x_data_all_1[:, 477 : 477 + features],\n",
    "            x_data_all_2[:, 256:],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    y_label_all = np.load(\n",
    "        \"y_label_handcraft.npy\", allow_pickle=True\n",
    "    )  # labels are the same, eigher handcraft or vgg is correct\n",
    "    y_uid_all = np.load(\"y_uid_handcraft.npy\", allow_pickle=True)\n",
    "\n",
    "    # split the data for different tasks\n",
    "    x_data_all_1 = x_data_all[y_label_all == 1]  #covidandroidnocough\n",
    "    x_data_all_2 = x_data_all[y_label_all == 2]  #covidandroidwithcough\n",
    "    x_data_all_3 = x_data_all[y_label_all == 3]  #covidwebnocough\n",
    "    x_data_all_4 = x_data_all[y_label_all == 4]  #covidwebwithcough\n",
    "    x_data_all_6 = x_data_all[y_label_all == 6]  #asthmaandroidwithcough\n",
    "    x_data_all_8 = x_data_all[y_label_all == 8]  #asthmawebwithcough\n",
    "    x_data_all_m1 = x_data_all[y_label_all == -1] #healthyandroidnosymp\n",
    "    x_data_all_m2 = x_data_all[y_label_all == -2] #healthyandroidwithcough\n",
    "    x_data_all_m3 = x_data_all[y_label_all == -3] #healthywebnosymp\n",
    "    x_data_all_m4 = x_data_all[y_label_all == -4] #healthywebwithcough\n",
    "\n",
    "    y_label_all_1 = y_label_all[y_label_all == 1]\n",
    "    y_label_all_2 = y_label_all[y_label_all == 2]\n",
    "    y_label_all_3 = y_label_all[y_label_all == 3]\n",
    "    y_label_all_4 = y_label_all[y_label_all == 4]\n",
    "    y_label_all_6 = y_label_all[y_label_all == 6]\n",
    "    y_label_all_8 = y_label_all[y_label_all == 8]\n",
    "    y_label_all_m1 = y_label_all[y_label_all == -1]\n",
    "    y_label_all_m2 = y_label_all[y_label_all == -2]\n",
    "    y_label_all_m3 = y_label_all[y_label_all == -3]\n",
    "    y_label_all_m4 = y_label_all[y_label_all == -4]\n",
    "\n",
    "    y_uid_1 = y_uid_all[y_label_all == 1]\n",
    "    y_uid_2 = y_uid_all[y_label_all == 2]\n",
    "    y_uid_3 = y_uid_all[y_label_all == 3]\n",
    "    y_uid_4 = y_uid_all[y_label_all == 4]\n",
    "    y_uid_6 = y_uid_all[y_label_all == 6]\n",
    "    y_uid_8 = y_uid_all[y_label_all == 8]\n",
    "    y_uid_m1 = y_uid_all[y_label_all == -1]\n",
    "    y_uid_m2 = y_uid_all[y_label_all == -2]\n",
    "    y_uid_m3 = y_uid_all[y_label_all == -3]\n",
    "    y_uid_m4 = y_uid_all[y_label_all == -4]\n",
    "\n",
    "    head = [\n",
    "        \"Tasks\",\n",
    "        \"Train\",\n",
    "        \"Test\",  \n",
    "        \"BreathingCough_PCA\",\n",
    "        \"BreathingCough_AUC\",\n",
    "        \"BreathingCough_ACC\",\n",
    "        \"BreathingCough_Pre\",\n",
    "        \"BreathingCough_Rec\",\n",
    "    ]\n",
    "\n",
    "    for i0 in [\"balanced\"]:  # downsample to balance for both train and test\n",
    "        for n in [0.95]:  # the variance remained after PCA\n",
    "            print(\"PCA:\", n)\n",
    "            for i1 in [\"task1\"]:\n",
    "                print(\"Conduct\", i1)\n",
    "                line = [i0 + str(n) + i1]\n",
    "                if i1 == \"task1\":\n",
    "                    x_data_all_task = np.concatenate(\n",
    "                        (\n",
    "                            x_data_all_1,\n",
    "                            x_data_all_2,\n",
    "                            x_data_all_3,\n",
    "                            x_data_all_4,\n",
    "                            x_data_all_m1,\n",
    "                            x_data_all_m3,\n",
    "                        ),\n",
    "                        axis=0,\n",
    "                    )\n",
    "                    y_label_all_task = np.concatenate(\n",
    "                        (\n",
    "                            y_label_all_1,\n",
    "                            y_label_all_2,\n",
    "                            y_label_all_3,\n",
    "                            y_label_all_4,\n",
    "                            y_label_all_m1,\n",
    "                            y_label_all_m3,\n",
    "                        ),\n",
    "                        axis=0,\n",
    "                    )\n",
    "                    y_uid_all_task = np.concatenate(\n",
    "                        (y_uid_1, y_uid_2, y_uid_3, y_uid_4, y_uid_m1, y_uid_m3), axis=0\n",
    "                    )\n",
    "\n",
    "                    y_label_all_task[y_label_all_task > 0] = 1  # covid positive\n",
    "                    y_label_all_task[y_label_all_task < 0] = 0\n",
    "\n",
    "                for i2 in [\"breath_cough\"]:  # multi-modal\n",
    "                    x_data_all_this = x_data_all_task[:, : Number * 2]\n",
    "\n",
    "                    dpca = []\n",
    "                    acc = []\n",
    "                    pre = []\n",
    "                    rec = []\n",
    "                    auc = []\n",
    "                    prauc = []\n",
    "                    train_ratio = []\n",
    "                    test_ratio = []\n",
    "\n",
    "                    for seed in [1, 2, 5, 10, 100, 200, 500, 1000, 2000, 5000]:\n",
    "\n",
    "                        gss = GroupShuffleSplit(\n",
    "                            n_splits=1, test_size=0.2, random_state=seed\n",
    "                        )\n",
    "                        idx1, idx2 = next(\n",
    "                            gss.split(x_data_all_this, groups=y_uid_all_task)\n",
    "                        )\n",
    "\n",
    "                        # Get the split DataFrames.\n",
    "                        train_x, test_x = x_data_all_this[idx1], x_data_all_this[idx2]\n",
    "                        y_train, y_test = y_label_all_task[idx1], y_label_all_task[idx2]\n",
    "                        uid_train, uid_test = y_uid_all_task[idx1], y_uid_all_task[idx2]\n",
    "\n",
    "                        # merge training samples\n",
    "                        if i1 != \"task1\":\n",
    "                            train_users = set(uid_train)\n",
    "\n",
    "                        train_x, y_train = RandomUnderSampler(train_x, y_train)\n",
    "                        test_x, y_test = RandomUnderSampler(test_x, y_test)\n",
    "\n",
    "                        # train_ratio.append(1.0*np.sum(y_train==1)/np.sum(y_train==0))\n",
    "                        train_ratio.append(train_x.shape[0])\n",
    "                        # test_ratio.append(1.0*np.sum(y_test==1)/np.sum(y_test==0))\n",
    "                        test_ratio.append(test_x.shape[0])\n",
    "\n",
    "                        scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "                        x_train_n = scaler.transform(train_x)\n",
    "                        x_test_n = scaler.transform(test_x)\n",
    "\n",
    "                        pca = decomposition.PCA(n)\n",
    "                        pca.fit(x_train_n)\n",
    "                        x_train_n_pca = pca.fit_transform(x_train_n)\n",
    "                        dpca.append(x_train_n_pca.shape[1])\n",
    "                        x_train_n_pca = pca.fit_transform(x_train_n)\n",
    "                        x_test_n_pca = pca.transform(x_test_n)\n",
    "\n",
    "                        # for SVM\n",
    "                        param_grid = [\n",
    "                            {\n",
    "                                \"C\": [10, 100, 1000],\n",
    "                                \"gamma\": [0.1, 0.01, 0.001, 0.0001],\n",
    "                                \"kernel\": [\"rbf\"],\n",
    "                                \"class_weight\": [\"balanced\"],\n",
    "                            }\n",
    "                        ]\n",
    "\n",
    "                        clf = SVC(probability=True)\n",
    "                        gs = GridSearchCV(\n",
    "                            clf,\n",
    "                            param_grid,\n",
    "                            scoring=make_scorer(roc_auc_score),\n",
    "                            n_jobs=-1,\n",
    "                            cv=5,\n",
    "                        )\n",
    "\n",
    "                        gs = gs.fit(x_train_n_pca, y_train)\n",
    "                        joblib.dump(gs.best_estimator_, \"best_model_android.pkl\")\n",
    "\n",
    "                        clf = joblib.load(\"best_model_android.pkl\")\n",
    "                        predicted = clf.predict(x_test_n_pca)\n",
    "                        probs = clf.predict_proba(x_test_n_pca)\n",
    "                        pre.append(metrics.precision_score(y_test, predicted))\n",
    "                        acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "                        auc.append(metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "                        precision, recall, _ = precision_recall_curve(\n",
    "                            y_test, probs[:, 1]\n",
    "                        )\n",
    "                        prauc.append(metrics.auc(recall, precision))\n",
    "                        rec.append(metrics.recall_score(y_test, predicted))\n",
    "\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(train_ratio)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(train_ratio)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(test_ratio)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(test_ratio)) + \")\"\n",
    "                    )\n",
    "\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(dpca)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(dpca)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(auc)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(auc)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(acc)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(acc)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(pre)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(pre)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(rec)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(rec)) + \")\"\n",
    "                    )\n",
    "\n",
    "    for i in range(len(line)):\n",
    "        print(head[i], line[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task2\n",
    "### Feature: Type 3, (A) denotes that we use VGGish-based feature plus duration, tempo, onset, and period.\n",
    "### Parameter: Cough with PCA =0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========feature group=======\n",
      "features dimension: 260\n",
      "PCA: 0.9\n",
      "Conduct task2\n",
      "Tasks balanced0.9task2\n",
      "Train 49.40(4.98)\n",
      "Test 10.00(4.73)\n",
      "Cough_PCA 29.10(2.02)\n",
      "Cough_AUC 0.8273(0.1775)\n",
      "Cough_ACC 0.7751(0.1622)\n",
      "Cough_Pre 0.8017(0.1578)\n",
      "Cough_Rec 0.7196(0.2347)\n"
     ]
    }
   ],
   "source": [
    "for features in [4]:  # different feature combination\n",
    "    print(\"========feature group=======\")\n",
    "    Number = features + 256  # the feature dimension for one modal\n",
    "    print(\"features dimension:\", Number)\n",
    "    x_data_all_1 = np.load(\"x_data_handcraft.npy\", allow_pickle=True)\n",
    "    x_data_all_2 = np.load(\"x_data_vgg.npy\", allow_pickle=True)\n",
    "    x_data_all_2 = np.squeeze(x_data_all_2)  # breath all, cough all, demo\n",
    "    # subset from handcraft for breath + vgg for breath + subset from handcraft for cough + vgg for cough\n",
    "    x_data_all = np.concatenate(\n",
    "        (\n",
    "            x_data_all_1[:, :features],\n",
    "            x_data_all_2[:, :256],\n",
    "            x_data_all_1[:, 477 : 477 + features],\n",
    "            x_data_all_2[:, 256:],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    y_label_all = np.load(\n",
    "        \"y_label_handcraft.npy\", allow_pickle=True\n",
    "    )  # labels are the same, eigher handcraft or vgg is correct\n",
    "    y_uid_all = np.load(\"y_uid_handcraft.npy\", allow_pickle=True)\n",
    "\n",
    "    # split the data for different tasks\n",
    "    x_data_all_1 = x_data_all[y_label_all == 1]  #covidandroidnocough\n",
    "    x_data_all_2 = x_data_all[y_label_all == 2]  #covidandroidwithcough\n",
    "    x_data_all_3 = x_data_all[y_label_all == 3]  #covidwebnocough\n",
    "    x_data_all_4 = x_data_all[y_label_all == 4]  #covidwebwithcough\n",
    "    x_data_all_6 = x_data_all[y_label_all == 6]  #asthmaandroidwithcough\n",
    "    x_data_all_8 = x_data_all[y_label_all == 8]  #asthmawebwithcough\n",
    "    x_data_all_m1 = x_data_all[y_label_all == -1] #healthyandroidnosymp\n",
    "    x_data_all_m2 = x_data_all[y_label_all == -2] #healthyandroidwithcough\n",
    "    x_data_all_m3 = x_data_all[y_label_all == -3] #healthywebnosymp\n",
    "    x_data_all_m4 = x_data_all[y_label_all == -4] #healthywebwithcough\n",
    "\n",
    "    y_label_all_1 = y_label_all[y_label_all == 1]\n",
    "    y_label_all_2 = y_label_all[y_label_all == 2]\n",
    "    y_label_all_3 = y_label_all[y_label_all == 3]\n",
    "    y_label_all_4 = y_label_all[y_label_all == 4]\n",
    "    y_label_all_6 = y_label_all[y_label_all == 6]\n",
    "    y_label_all_8 = y_label_all[y_label_all == 8]\n",
    "    y_label_all_m1 = y_label_all[y_label_all == -1]\n",
    "    y_label_all_m2 = y_label_all[y_label_all == -2]\n",
    "    y_label_all_m3 = y_label_all[y_label_all == -3]\n",
    "    y_label_all_m4 = y_label_all[y_label_all == -4]\n",
    "\n",
    "    y_uid_1 = y_uid_all[y_label_all == 1]\n",
    "    y_uid_2 = y_uid_all[y_label_all == 2]\n",
    "    y_uid_3 = y_uid_all[y_label_all == 3]\n",
    "    y_uid_4 = y_uid_all[y_label_all == 4]\n",
    "    y_uid_6 = y_uid_all[y_label_all == 6]\n",
    "    y_uid_8 = y_uid_all[y_label_all == 8]\n",
    "    y_uid_m1 = y_uid_all[y_label_all == -1]\n",
    "    y_uid_m2 = y_uid_all[y_label_all == -2]\n",
    "    y_uid_m3 = y_uid_all[y_label_all == -3]\n",
    "    y_uid_m4 = y_uid_all[y_label_all == -4]\n",
    "\n",
    "    head = [\n",
    "        \"Tasks\",\n",
    "        \"Train\",\n",
    "        \"Test\", \n",
    "        \"Cough_PCA\",\n",
    "        \"Cough_AUC\",\n",
    "        \"Cough_ACC\",\n",
    "        \"Cough_Pre\",\n",
    "        \"Cough_Rec\",\n",
    "    ]\n",
    "\n",
    "    for i0 in [\"balanced\"]:  # downsample to balance for both train and test\n",
    "        for n in [0.9]:  # the variance remained after PCA\n",
    "            print(\"PCA:\", n)\n",
    "            for i1 in [\"task2\"]:\n",
    "                print(\"Conduct\", i1)\n",
    "                line = [i0 + str(n) + i1]\n",
    "                if i1 == \"task2\":\n",
    "                    x_data_all_task = np.concatenate(\n",
    "                        (x_data_all_2, x_data_all_4, x_data_all_m2, x_data_all_m4),\n",
    "                        axis=0,\n",
    "                    )\n",
    "                    y_label_all_task = np.concatenate(\n",
    "                        (y_label_all_2, y_label_all_4, y_label_all_m2, y_label_all_m4),\n",
    "                        axis=0,\n",
    "                    )\n",
    "                    y_uid_all_task = np.concatenate(\n",
    "                        (y_uid_2, y_uid_4, y_uid_m2, y_uid_m4), axis=0\n",
    "                    )\n",
    "\n",
    "                    y_label_all_task[y_label_all_task > 0] = 1  # covid positive\n",
    "                    y_label_all_task[y_label_all_task < 0] = 0\n",
    "\n",
    "                for i2 in [\"cough\"]:  # multi-modal\n",
    "                    x_data_all_this = x_data_all_task[:, Number : Number * 2]\n",
    "\n",
    "                    dpca = []\n",
    "                    acc = []\n",
    "                    pre = []\n",
    "                    rec = []\n",
    "                    auc = []\n",
    "                    prauc = []\n",
    "                    train_ratio = []\n",
    "                    test_ratio = []\n",
    "\n",
    "                    for seed in [1, 2, 5, 10, 100, 200, 500, 1000, 2000, 5000]:\n",
    "\n",
    "                        gss = GroupShuffleSplit(\n",
    "                            n_splits=1, test_size=0.2, random_state=seed\n",
    "                        )\n",
    "                        idx1, idx2 = next(\n",
    "                            gss.split(x_data_all_this, groups=y_uid_all_task)\n",
    "                        )\n",
    "\n",
    "                        # Get the split DataFrames.\n",
    "                        train_x, test_x = x_data_all_this[idx1], x_data_all_this[idx2]\n",
    "                        y_train, y_test = y_label_all_task[idx1], y_label_all_task[idx2]\n",
    "                        uid_train, uid_test = y_uid_all_task[idx1], y_uid_all_task[idx2]\n",
    "\n",
    "                        # merge training samples\n",
    "                        if i1 != \"task1\":\n",
    "                            train_users = set(uid_train)\n",
    "\n",
    "                        train_x, y_train = RandomUnderSampler(train_x, y_train)\n",
    "                        test_x, y_test = RandomUnderSampler(test_x, y_test)\n",
    "\n",
    "                        # train_ratio.append(1.0*np.sum(y_train==1)/np.sum(y_train==0))\n",
    "                        train_ratio.append(train_x.shape[0])\n",
    "                        # test_ratio.append(1.0*np.sum(y_test==1)/np.sum(y_test==0))\n",
    "                        test_ratio.append(test_x.shape[0])\n",
    "\n",
    "                        scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "                        x_train_n = scaler.transform(train_x)\n",
    "                        x_test_n = scaler.transform(test_x)\n",
    "\n",
    "                        pca = decomposition.PCA(n)\n",
    "                        pca.fit(x_train_n)\n",
    "                        x_train_n_pca = pca.fit_transform(x_train_n)\n",
    "                        dpca.append(x_train_n_pca.shape[1])\n",
    "                        x_train_n_pca = pca.fit_transform(x_train_n)\n",
    "                        x_test_n_pca = pca.transform(x_test_n)\n",
    "\n",
    "                        # for SVM\n",
    "                        param_grid = [\n",
    "                            {\n",
    "                                \"C\": [10, 100, 1000],\n",
    "                                \"gamma\": [0.1, 0.01, 0.001, 0.0001],\n",
    "                                \"kernel\": [\"rbf\"],\n",
    "                                \"class_weight\": [\"balanced\"],\n",
    "                            }\n",
    "                        ]\n",
    "\n",
    "                        clf = SVC(probability=True)\n",
    "                        gs = GridSearchCV(\n",
    "                            clf,\n",
    "                            param_grid,\n",
    "                            scoring=make_scorer(roc_auc_score),\n",
    "                            n_jobs=-1,\n",
    "                            cv=5,\n",
    "                        )\n",
    "\n",
    "                        gs = gs.fit(x_train_n_pca, y_train)\n",
    "                        joblib.dump(gs.best_estimator_, \"best_model_android.pkl\")\n",
    "\n",
    "                        clf = joblib.load(\"best_model_android.pkl\")\n",
    "                        predicted = clf.predict(x_test_n_pca)\n",
    "                        probs = clf.predict_proba(x_test_n_pca)\n",
    "                        pre.append(metrics.precision_score(y_test, predicted))\n",
    "                        acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "                        auc.append(metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "                        precision, recall, _ = precision_recall_curve(\n",
    "                            y_test, probs[:, 1]\n",
    "                        )\n",
    "                        prauc.append(metrics.auc(recall, precision))\n",
    "                        rec.append(metrics.recall_score(y_test, predicted))\n",
    "\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(train_ratio)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(train_ratio)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(test_ratio)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(test_ratio)) + \")\"\n",
    "                    )\n",
    "\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(dpca)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(dpca)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(auc)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(auc)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(acc)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(acc)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(pre)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(pre)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(rec)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(rec)) + \")\"\n",
    "                    )\n",
    "\n",
    "    for i in range(len(line)):\n",
    "        print(head[i], line[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task3\n",
    "### Feature: Type 3,(B) for all features except Δ-MFCCs and Δ2-MFCCs,\n",
    "### Parameter: Breath with PCA =0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========feature group=======\n",
      "features dimension: 447\n",
      "PCA: 0.7\n",
      "Conduct task3\n",
      "Tasks balanced0.7task3\n",
      "Train 32.00(3.46)\n",
      "Test 8.00(3.46)\n",
      "Breathing_PCA 9.60(0.66)\n",
      "Breathing_AUC 0.8048(0.1434)\n",
      "Breathing_ACC 0.7029(0.2151)\n",
      "Breathing_Pre 0.6939(0.2094)\n",
      "Breathing_Rec 0.6933(0.2604)\n"
     ]
    }
   ],
   "source": [
    "for features in [191]:  # different feature combination\n",
    "    print(\"========feature group=======\")\n",
    "    Number = features + 256  # the feature dimension for one modal\n",
    "    print(\"features dimension:\", Number)\n",
    "    x_data_all_1 = np.load(\"x_data_handcraft.npy\", allow_pickle=True)\n",
    "    x_data_all_2 = np.load(\"x_data_vgg.npy\", allow_pickle=True)\n",
    "    x_data_all_2 = np.squeeze(x_data_all_2)  # breath all, cough all, demo\n",
    "    # subset from handcraft for breath + vgg for breath + subset from handcraft for cough + vgg for cough\n",
    "    x_data_all = np.concatenate(\n",
    "        (\n",
    "            x_data_all_1[:, :features],\n",
    "            x_data_all_2[:, :256],\n",
    "            x_data_all_1[:, 477 : 477 + features],\n",
    "            x_data_all_2[:, 256:],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    y_label_all = np.load(\n",
    "        \"y_label_handcraft.npy\", allow_pickle=True\n",
    "    )  # labels are the same, eigher handcraft or vgg is correct\n",
    "    y_uid_all = np.load(\"y_uid_handcraft.npy\", allow_pickle=True)\n",
    "\n",
    "    # split the data for different tasks\n",
    "    x_data_all_1 = x_data_all[y_label_all == 1]  #covidandroidnocough\n",
    "    x_data_all_2 = x_data_all[y_label_all == 2]  #covidandroidwithcough\n",
    "    x_data_all_3 = x_data_all[y_label_all == 3]  #covidwebnocough\n",
    "    x_data_all_4 = x_data_all[y_label_all == 4]  #covidwebwithcough\n",
    "    x_data_all_6 = x_data_all[y_label_all == 6]  #asthmaandroidwithcough\n",
    "    x_data_all_8 = x_data_all[y_label_all == 8]  #asthmawebwithcough\n",
    "    x_data_all_m1 = x_data_all[y_label_all == -1] #healthyandroidnosymp\n",
    "    x_data_all_m2 = x_data_all[y_label_all == -2] #healthyandroidwithcough\n",
    "    x_data_all_m3 = x_data_all[y_label_all == -3] #healthywebnosymp\n",
    "    x_data_all_m4 = x_data_all[y_label_all == -4] #healthywebwithcough\n",
    "\n",
    "    y_label_all_1 = y_label_all[y_label_all == 1]\n",
    "    y_label_all_2 = y_label_all[y_label_all == 2]\n",
    "    y_label_all_3 = y_label_all[y_label_all == 3]\n",
    "    y_label_all_4 = y_label_all[y_label_all == 4]\n",
    "    y_label_all_6 = y_label_all[y_label_all == 6]\n",
    "    y_label_all_8 = y_label_all[y_label_all == 8]\n",
    "    y_label_all_m1 = y_label_all[y_label_all == -1]\n",
    "    y_label_all_m2 = y_label_all[y_label_all == -2]\n",
    "    y_label_all_m3 = y_label_all[y_label_all == -3]\n",
    "    y_label_all_m4 = y_label_all[y_label_all == -4]\n",
    "\n",
    "    y_uid_1 = y_uid_all[y_label_all == 1]\n",
    "    y_uid_2 = y_uid_all[y_label_all == 2]\n",
    "    y_uid_3 = y_uid_all[y_label_all == 3]\n",
    "    y_uid_4 = y_uid_all[y_label_all == 4]\n",
    "    y_uid_6 = y_uid_all[y_label_all == 6]\n",
    "    y_uid_8 = y_uid_all[y_label_all == 8]\n",
    "    y_uid_m1 = y_uid_all[y_label_all == -1]\n",
    "    y_uid_m2 = y_uid_all[y_label_all == -2]\n",
    "    y_uid_m3 = y_uid_all[y_label_all == -3]\n",
    "    y_uid_m4 = y_uid_all[y_label_all == -4]\n",
    "\n",
    "    head = [\n",
    "        \"Tasks\",\n",
    "        \"Train\",\n",
    "        \"Test\",\n",
    "        \"Breathing_PCA\",\n",
    "        \"Breathing_AUC\",\n",
    "        \"Breathing_ACC\",\n",
    "        \"Breathing_Pre\",\n",
    "        \"Breathing_Rec\",\n",
    "        #'Cough_PCA','Cough_AUC','Cough_ACC','Cough_Pre','Cough_Rec',\n",
    "        #'BreathingCough_PCA','BreathingCough_AUC','BreathingCough_ACC','BreathingCough_Pre','BreathingCough_Rec'\n",
    "    ]\n",
    "\n",
    "    for i0 in [\"balanced\"]:  # downsample to balance for both train and test\n",
    "        for n in [0.7]:  # the variance remained after PCA\n",
    "            print(\"PCA:\", n)\n",
    "            for i1 in [\"task3\"]:\n",
    "                print(\"Conduct\", i1)\n",
    "                line = [i0 + str(n) + i1]\n",
    "                if i1 == \"task3\":\n",
    "                    x_data_all_task = np.concatenate(\n",
    "                        (x_data_all_2, x_data_all_4, x_data_all_6, x_data_all_8), axis=0\n",
    "                    )\n",
    "                    y_label_all_task = np.concatenate(\n",
    "                        (y_label_all_2, y_label_all_4, y_label_all_6, y_label_all_8),\n",
    "                        axis=0,\n",
    "                    )\n",
    "                    y_uid_all_task = np.concatenate(\n",
    "                        (y_uid_2, y_uid_4, y_uid_6, y_uid_8), axis=0\n",
    "                    )\n",
    "\n",
    "                    y_label_all_task[y_label_all_task < 5] = 1  # covid positive\n",
    "                    y_label_all_task[y_label_all_task > 4] = 0\n",
    "\n",
    "                for i2 in [\"breath\"]:  # multi-modal\n",
    "                    x_data_all_this = x_data_all_task[:, 0:Number]\n",
    "\n",
    "                    dpca = []\n",
    "                    acc = []\n",
    "                    pre = []\n",
    "                    rec = []\n",
    "                    auc = []\n",
    "                    prauc = []\n",
    "                    train_ratio = []\n",
    "                    test_ratio = []\n",
    "\n",
    "                    for seed in [1, 2, 5, 10, 100, 200, 500, 1000, 2000, 5000]:\n",
    "\n",
    "                        gss = GroupShuffleSplit(\n",
    "                            n_splits=1, test_size=0.2, random_state=seed\n",
    "                        )\n",
    "                        idx1, idx2 = next(\n",
    "                            gss.split(x_data_all_this, groups=y_uid_all_task)\n",
    "                        )\n",
    "\n",
    "                        # Get the split DataFrames.\n",
    "                        train_x, test_x = x_data_all_this[idx1], x_data_all_this[idx2]\n",
    "                        y_train, y_test = y_label_all_task[idx1], y_label_all_task[idx2]\n",
    "                        uid_train, uid_test = y_uid_all_task[idx1], y_uid_all_task[idx2]\n",
    "\n",
    "                        # merge training samples\n",
    "                        if i1 != \"task1\":\n",
    "                            train_users = set(uid_train)\n",
    "\n",
    "                        train_x, y_train = RandomUnderSampler(train_x, y_train)\n",
    "                        test_x, y_test = RandomUnderSampler(test_x, y_test)\n",
    "\n",
    "                        # train_ratio.append(1.0*np.sum(y_train==1)/np.sum(y_train==0))\n",
    "                        train_ratio.append(train_x.shape[0])\n",
    "                        # test_ratio.append(1.0*np.sum(y_test==1)/np.sum(y_test==0))\n",
    "                        test_ratio.append(test_x.shape[0])\n",
    "\n",
    "                        scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "                        x_train_n = scaler.transform(train_x)\n",
    "                        x_test_n = scaler.transform(test_x)\n",
    "\n",
    "                        pca = decomposition.PCA(n)\n",
    "                        pca.fit(x_train_n)\n",
    "                        x_train_n_pca = pca.fit_transform(x_train_n)\n",
    "                        dpca.append(x_train_n_pca.shape[1])\n",
    "                        x_train_n_pca = pca.fit_transform(x_train_n)\n",
    "                        x_test_n_pca = pca.transform(x_test_n)\n",
    "\n",
    "                        # for SVM\n",
    "                        param_grid = [\n",
    "                            {\n",
    "                                \"C\": [10, 100, 1000],\n",
    "                                \"gamma\": [0.1, 0.01, 0.001, 0.0001],\n",
    "                                \"kernel\": [\"rbf\"],\n",
    "                                \"class_weight\": [\"balanced\"],\n",
    "                            }\n",
    "                        ]\n",
    "\n",
    "                        clf = SVC(probability=True)\n",
    "                        gs = GridSearchCV(\n",
    "                            clf,\n",
    "                            param_grid,\n",
    "                            scoring=make_scorer(roc_auc_score),\n",
    "                            n_jobs=-1,\n",
    "                            cv=5,\n",
    "                        )\n",
    "\n",
    "                        gs = gs.fit(x_train_n_pca, y_train)\n",
    "                        joblib.dump(gs.best_estimator_, \"best_model_android.pkl\")\n",
    "\n",
    "                        clf = joblib.load(\"best_model_android.pkl\")\n",
    "                        predicted = clf.predict(x_test_n_pca)\n",
    "                        probs = clf.predict_proba(x_test_n_pca)\n",
    "                        pre.append(metrics.precision_score(y_test, predicted))\n",
    "                        acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "                        auc.append(metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "                        precision, recall, _ = precision_recall_curve(\n",
    "                            y_test, probs[:, 1]\n",
    "                        )\n",
    "                        prauc.append(metrics.auc(recall, precision))\n",
    "                        rec.append(metrics.recall_score(y_test, predicted))\n",
    "\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(train_ratio)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(train_ratio)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(test_ratio)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(test_ratio)) + \")\"\n",
    "                    )\n",
    "\n",
    "                    line.append(\n",
    "                        \"{:.2f}\".format(np.mean(dpca)) + \"(\"\n",
    "                        \"{:.2f}\".format(np.std(dpca)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(auc)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(auc)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(acc)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(acc)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(pre)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(pre)) + \")\"\n",
    "                    )\n",
    "                    line.append(\n",
    "                        \"{:.4f}\".format(np.mean(rec)) + \"(\"\n",
    "                        \"{:.4f}\".format(np.std(rec)) + \")\"\n",
    "                    )\n",
    "\n",
    "    for i in range(len(line)):\n",
    "        print(head[i], line[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
